#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
High Probability Live Trading Signal Generator (Refined for Swing Trading)

This script:
1. Loads long-term historical symbol performance data.
2. Scans specified stock lists (European, S&P 500, S&P 400) for
   high probability setups using yfinance for the current day.
3. Applies a strict win_rate_threshold (e.g., 0.9).
4. Selects a maximum of ONE trade candidate per day, using historical
   performance as a tie-breaker if multiple setups have the same high score.
5. Outputs the details of the selected trade for potential notification.

User manages weekly trade limits (e.g., max 3 per week) manually.
"""

import os
import logging
import warnings
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import yfinance as yf
from tqdm import tqdm
from collections import OrderedDict 
from typing import Any # Add this import
from pathlib import Path
import json # Added for JSON operations

# Suppress warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

logger.info(f"--- PATH DIAGNOSTICS ---")
logger.info(f"Current Working Directory (os.getcwd()): {os.getcwd()}")
logger.info(f"__file__: {__file__}")
logger.info(f"os.path.abspath(__file__): {os.path.abspath(__file__)}")
# The following SCRIPT_DIR, PROJECT_BASE_DIR, DATA_DIR are already defined globally a bit lower.
# We will log their values after they are defined.
logger.info(f"--- END PATH DIAGNOSTICS ---")

# --- Path Constants ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_BASE_DIR = os.path.dirname(SCRIPT_DIR)
DATA_DIR = os.path.join(PROJECT_BASE_DIR, 'Data')

logger.info(f"--- RESOLVED PATH CONSTANTS ---")
logger.info(f"SCRIPT_DIR: {SCRIPT_DIR}")
logger.info(f"PROJECT_BASE_DIR: {PROJECT_BASE_DIR}")
logger.info(f"DATA_DIR: {DATA_DIR}")
logger.info(f"--- END RESOLVED PATH CONSTANTS ---")

# --- Path to historical performance file (generated by create_historical_performance.py) ---
LONG_TERM_PERF_RESULTS_DIR = os.path.join(PROJECT_BASE_DIR, 'results', 'long_term_historical_perf')
HISTORICAL_STATS_FOR_RANKING_FILE = os.path.join(LONG_TERM_PERF_RESULTS_DIR, 'historical_symbol_performance_long_term.csv')

TICKER_FILES = {
    'euro': os.path.join(DATA_DIR, 'euro_tickers.csv'),
    'sp500': os.path.join(DATA_DIR, 'sp500_tickers.csv'),
    'sp400': os.path.join(DATA_DIR, 'sp400_tickers.csv'),
}

DEFAULT_SYMBOLS = { # Shortened for brevity
    'euro': ['ASML.AS', 'SAP.DE'], 'sp500': ['AAPL', 'MSFT'], 'sp400': ['XPO', 'HUBB']
} 

# --- MODIFIED: Strategy parameters for live scanning ---
STRATEGY_PARAMS = {
    'win_rate_threshold': 0.9, # Set to 0.9 as per backtest insights
    'risk_reward_ratio_threshold': 1.5, # Should match backtester
    'atr_multiplier': 1.5,
    'target_multiplier': 2.0,
    'volume_spike_threshold': 2.0,
    'prioritize_volume_spike': True, 
    'prioritize_low_tier': True,    
    'min_data_days': 250 
}

DATA_FETCH_PERIOD = "2y" # How much historical data to fetch for indicator calculation

# Define SCORE_BINS for trade distribution summary
SCORE_BINS = [
    (0.0, 0.7), 
    (0.7, 0.8), 
    (0.8, 0.9), 
    (0.9, 0.95), 
    (0.95, 1.01) # Upper bound is exclusive, so 1.01 to include 1.0
]

# Global DataFrame for historical performance
long_term_historical_perf_df = None


def load_long_term_historical_stats():
    """Loads the long-term historical symbol performance data."""
    global long_term_historical_perf_df
    if os.path.exists(HISTORICAL_STATS_FOR_RANKING_FILE):
        try:
            long_term_historical_perf_df = pd.read_csv(HISTORICAL_STATS_FOR_RANKING_FILE)
            logger.info(f"Successfully loaded long-term historical symbol stats from {HISTORICAL_STATS_FOR_RANKING_FILE}")
            if not all(col in long_term_historical_perf_df.columns for col in ['symbol', 'hist_strength_score', 'hist_win_rate', 'hist_total_trades']):
                logger.warning(f"{HISTORICAL_STATS_FOR_RANKING_FILE} is missing expected columns for tie-breaking. Results may not use full historical context.")
        except Exception as e:
            logger.error(f"Error loading long-term historical symbol stats: {e}")
            long_term_historical_perf_df = None
    else:
        logger.warning(f"Long-term historical stats file not found: {HISTORICAL_STATS_FOR_RANKING_FILE}. Proceeding without historical tie-breaking.")
        long_term_historical_perf_df = None

# --- Helper Functions (load_symbols, fetch_stock_data_yf, calculate_indicators, detect_setup, calculate_potential_trade_params, apply_high_probability_filter_live) ---
# These should be identical to the robust versions in your create_historical_performance.py or the latest high_current.py
# For brevity, I will include the ones from the last version of high_current.py that included score distribution.
# Ensure these are robust and match the logic used in your backtests if you want consistency.

def load_symbols():
    all_symbols_by_market = {} # Initialize dictionary to store symbols by market

    logger.info(f"Attempting to list contents of DATA_DIR ({DATA_DIR})...")
    try:
        data_dir_contents = os.listdir(DATA_DIR)
        logger.info(f"Contents of DATA_DIR ({DATA_DIR}): {data_dir_contents}")
        if not data_dir_contents:
            logger.warning(f"DATA_DIR ({DATA_DIR}) is empty.")
    except FileNotFoundError:
        logger.error(f"DATA_DIR ({DATA_DIR}) not found when trying to list contents.")
    except Exception as e:
        logger.error(f"Error listing contents of DATA_DIR ({DATA_DIR}): {e}")

    for market, filename in TICKER_FILES.items():
        market_symbols = set() # Use a set for uniqueness within the market before converting to list
        loaded_from_file = False
        
        try:
            logger.info(f"For market {market.upper()}, checking existence of file at exact path: '{filename}'")
            if os.path.exists(filename):
                df_tickers = pd.read_csv(filename, header=0)
                symbols_from_file_list = []
                if not df_tickers.empty:
                    df_cols_lower = [str(col).lower() for col in df_tickers.columns]
                    if 'symbol' in df_cols_lower:
                        symbol_col_name = df_tickers.columns[df_cols_lower.index('symbol')]
                        symbols_from_file_list = df_tickers[symbol_col_name].tolist()
                    else: # Use first column if 'symbol' not found
                        symbols_from_file_list = df_tickers.iloc[:, 0].tolist()
                
                # Clean and filter symbols from the file list
                current_file_symbols = {str(s).strip() for s in symbols_from_file_list if isinstance(s, (str, float, int)) and str(s).strip()}
                
                if current_file_symbols:
                    # logger.info(f"Loaded {len(current_file_symbols)} symbols for {market.upper()} market from {filename}") # Old log
                    market_symbols.update(current_file_symbols)
                    loaded_from_file = True
                    logger.info(f"Market {market.upper()}: Loaded {len(market_symbols)} symbols from file.")
                else:
                    logger.warning(f"{market.upper()} tickers file '{filename}' was present but empty or contained no valid symbols.")
            else:
                logger.info(f"{market.upper()} tickers file '{filename}' not found.")

            # Fallback to default symbols if no symbols were loaded from file
            if not loaded_from_file:
                # logger.info(f"Attempting to use default list for {market.upper()} symbols.") # Old log
                default_market_symbols = DEFAULT_SYMBOLS.get(market, [])
                # Clean and filter default symbols
                cleaned_default_symbols = {str(s).strip() for s in default_market_symbols if isinstance(s, (str, float, int)) and str(s).strip()}

                if cleaned_default_symbols:
                    # logger.info(f"Using {len(cleaned_default_symbols)} default symbols for {market.upper()} market.") # Old log
                    market_symbols.update(cleaned_default_symbols)
                    logger.info(f"Market {market.upper()}: Fell back to {len(cleaned_default_symbols)} default symbols.")
                else:
                    logger.warning(f"No default symbols found or provided for {market.upper()} market.")
                    logger.info(f"Market {market.upper()}: Fell back to 0 default symbols (none available/valid).")
        
        except pd.errors.EmptyDataError:
            logger.warning(f"Pandas EmptyDataError: {market.upper()} tickers file '{filename}' is empty. Trying defaults.")
            if not loaded_from_file:
                default_market_symbols = DEFAULT_SYMBOLS.get(market, [])
                cleaned_default_symbols = {str(s).strip() for s in default_market_symbols if isinstance(s, (str, float, int)) and str(s).strip()}
                if cleaned_default_symbols:
                    # logger.info(f"Using {len(cleaned_default_symbols)} default symbols for {market.upper()} market due to EmptyDataError.") # Old log
                    market_symbols.update(cleaned_default_symbols)
                    logger.info(f"Market {market.upper()}: Fell back to {len(cleaned_default_symbols)} default symbols after EmptyDataError.")
                else:
                    logger.warning(f"No default symbols found for {market.upper()} market after EmptyDataError.")
                    logger.info(f"Market {market.upper()}: Fell back to 0 default symbols after EmptyDataError (none available/valid).")

        except Exception as e:
            logger.error(f"Error processing {market.upper()} symbols (file: {filename}): {e}. Attempting to use defaults if available.")
            if not loaded_from_file:
                default_market_symbols = DEFAULT_SYMBOLS.get(market, [])
                cleaned_default_symbols = {str(s).strip() for s in default_market_symbols if isinstance(s, (str, float, int)) and str(s).strip()}
                if cleaned_default_symbols:
                    # logger.info(f"Using {len(cleaned_default_symbols)} default symbols for {market.upper()} market due to error: {e}.") # Old log
                    market_symbols.update(cleaned_default_symbols)
                    logger.info(f"Market {market.upper()}: Fell back to {len(cleaned_default_symbols)} default symbols after error: {e}.")
                else:
                    logger.warning(f"No default symbols found for {market.upper()} market after error: {e}.")
                    logger.info(f"Market {market.upper()}: Fell back to 0 default symbols after error (none available/valid).")
        
        all_symbols_by_market[market] = sorted(list(market_symbols))
        if not all_symbols_by_market[market]:
             logger.warning(f"No symbols loaded for {market.upper()} market (neither from file nor defaults).")
        # Removed the per-market log here as it's covered by the new logs or the summary below.

    # Check if any symbols were loaded at all across all markets
    # Consolidate all unique symbols from all markets
    all_unique_symbols = set()
    for symbols_list in all_symbols_by_market.values():
        all_unique_symbols.update(symbols_list)
    total_unique_symbols_loaded = len(all_unique_symbols)

    if total_unique_symbols_loaded == 0:
        logger.error("CRITICAL: No unique symbols loaded across any market segments. Exiting or returning empty structure.")
        return all_symbols_by_market

    logger.info(f"Symbol loading complete. Total unique symbols loaded across all markets: {total_unique_symbols_loaded}.")
    # The old per-market summary log is removed as individual market loading is now logged.
    # for market, symbols in all_symbols_by_market.items():
    #     logger.info(f"  {market.upper()}: {len(symbols)} symbols loaded.") # Redundant with new logs
        
    return all_symbols_by_market

def fetch_stock_data_yf(symbol, period=DATA_FETCH_PERIOD):
    try:
        ticker = yf.Ticker(symbol)
        df = ticker.history(period=period, auto_adjust=True) 
        if df.empty: logger.warning(f"No data for {symbol} via yfinance for {period}."); return None
        df.rename(columns={'Open': 'Open', 'High': 'High', 'Low': 'Low', 'Close': 'Close', 'Volume': 'Volume'}, inplace=True, errors='ignore')
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        if not all(col in df.columns for col in required_columns):
            logger.error(f"Missing required columns in {symbol} yf data (got {list(df.columns)})"); return None
        for col in required_columns: df[col] = pd.to_numeric(df[col], errors='coerce')
        df.dropna(subset=required_columns, inplace=True); df.sort_index(inplace=True)
        if len(df) < STRATEGY_PARAMS['min_data_days']:
            logger.warning(f"Not enough data for {symbol} ({len(df)} days). Need {STRATEGY_PARAMS['min_data_days']}."); return None
        return df
    except Exception as e: logger.error(f"Error fetching data for {symbol}: {e}"); return None

def calculate_indicators(df):
    # (This should be the same robust version as in create_historical_performance.py)
    df_indicators = df.copy()
    
    # Check for essential raw data columns
    essential_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
    missing_essential_cols = [col for col in essential_cols if col not in df_indicators.columns]
    if missing_essential_cols:
        logger.debug(f"Input DataFrame for indicator calculation is missing essential columns: {missing_essential_cols}. Subsequent calculations may result in NaNs.")

    for period in [5, 10, 20, 50, 200]:
        if 'Close' in df_indicators.columns and len(df_indicators['Close']) >= period:
            df_indicators[f'ma{period}'] = df_indicators['Close'].rolling(window=period, min_periods=period).mean()
        else: df_indicators[f'ma{period}'] = np.nan
    if 'ma5' in df_indicators.columns and not df_indicators['ma5'].isnull().all():
        df_indicators['ma5_slope'] = df_indicators['ma5'].diff() / df_indicators['ma5'].shift(1)
        if 'ma5_slope' in df_indicators.columns and not df_indicators['ma5_slope'].isnull().all(): 
             df_indicators['ma5_slope_3day'] = df_indicators['ma5_slope'].rolling(window=3, min_periods=1).mean()
             df_indicators['ma5_slope_5day'] = df_indicators['ma5_slope'].rolling(window=5, min_periods=1).mean()
        else: df_indicators['ma5_slope_3day'], df_indicators['ma5_slope_5day'] = np.nan, np.nan
        df_indicators['ma5_slope_up'] = (df_indicators.get('ma5_slope', pd.Series(dtype=float)) > 0).astype(int)
        df_indicators['ma5_above_price'] = (df_indicators['ma5'] > df_indicators['Close']).astype(int)
    else:
        for col in ['ma5_slope','ma5_slope_3day','ma5_slope_5day','ma5_slope_up','ma5_above_price']: df_indicators[col]=np.nan
    if 'Close' in df_indicators.columns and not df_indicators['Close'].isnull().all():
        delta = df_indicators['Close'].diff()
        gain = delta.where(delta > 0, 0).rolling(window=14, min_periods=1).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()
        rs = gain / loss.replace(0, np.nan); df_indicators['rsi'] = 100 - (100 / (1 + rs)); df_indicators['rsi'].fillna(50, inplace=True) 
        df_indicators['rsi_change'] = df_indicators['rsi'].diff(); df_indicators['rsi_oversold'] = (df_indicators['rsi'] < 30).astype(int)
        if 'High' in df.columns and 'Low' in df.columns: 
            high_low = df_indicators['High'] - df_indicators['Low']; high_close = np.abs(df_indicators['High'] - df_indicators['Close'].shift())
            low_close = np.abs(df_indicators['Low'] - df_indicators['Close'].shift()); ranges = pd.concat([high_low, high_close, low_close], axis=1)
            true_range = ranges.max(axis=1, skipna=False); df_indicators['ATR'] = true_range.rolling(window=14, min_periods=14).mean()
        else: df_indicators['ATR'] = np.nan
    else: 
        for col in ['rsi','rsi_change','rsi_oversold','ATR']: df_indicators[col]=np.nan
    if 'Volume' in df_indicators.columns and 'Close' in df_indicators.columns and not df_indicators['Volume'].isnull().all() and df_indicators['Volume'].sum(skipna=True) > 0 :
        cumulative_volume = df_indicators['Volume'].cumsum()
        df_indicators['VWAP'] = np.where(cumulative_volume > 0, (df_indicators['Close'] * df_indicators['Volume']).cumsum() / cumulative_volume, np.nan)
        rolling_mean_volume = df_indicators['Volume'].rolling(window=20, min_periods=1).mean()
        df_indicators['volume_ratio'] = np.where(rolling_mean_volume > 0, df_indicators['Volume'] / rolling_mean_volume, np.nan)
        df_indicators['prev_day_volume_ratio'] = df_indicators['volume_ratio'].shift(1)
        df_indicators['volume_increasing'] = (df_indicators['Volume'] > df_indicators['Volume'].shift(1)).astype(int)
    else:
        for col in ['VWAP','volume_ratio','prev_day_volume_ratio','volume_increasing']: df_indicators[col]=np.nan
    if 'Close' in df_indicators.columns:
        for p in [1,2,3]: df_indicators[f'prev{p}_close_change'] = df_indicators['Close'].pct_change(p)
        df_indicators['close_vs_prev_close'] = df_indicators['Close'].pct_change()
    if 'Open' in df_indicators.columns and 'Close' in df_indicators.columns and not df_indicators['Open'].isnull().all() and not df_indicators['Open'].eq(0).all(): 
        df_indicators['close_vs_open'] = (df_indicators['Close'] - df_indicators['Open']) / df_indicators['Open'].replace(0, np.nan)
    else: df_indicators['close_vs_open'] = np.nan
    if 'High' in df_indicators.columns and 'Low' in df_indicators.columns and not df_indicators['Low'].isnull().all() and not df_indicators['Low'].eq(0).all(): 
        df_indicators['high_vs_low_range'] = (df_indicators['High'] - df_indicators['Low']) / df_indicators['Low'].replace(0, np.nan)
    else: df_indicators['high_vs_low_range'] = np.nan
    if all(x in df_indicators.columns and not df_indicators[x].isnull().all() for x in ['ma20', 'ma50', 'ma5', 'ma200']):
        df_indicators['day_thick_line_value'] = df_indicators['ma20'] - df_indicators['ma50']
        df_indicators['day_thick_line_slope'] = df_indicators['day_thick_line_value'].diff()
        df_indicators['day_thick_line_green'] = (df_indicators['day_thick_line_value'] > 0).astype(int)
        df_indicators['day_hist_value'] = df_indicators['ma5'] - df_indicators['ma20']
        df_indicators['day_hist_green'] = (df_indicators['day_hist_value'] > 0).astype(int)
        df_indicators['week_thick_line_value'] = df_indicators['ma50'] - df_indicators['ma200']
        df_indicators['week_red_below_zero_turns_green'] = ((df_indicators['week_thick_line_value'].shift(1) < 0) & (df_indicators['week_thick_line_value'] > 0)).astype(int)
    else:
        for col in ['day_thick_line_value','day_thick_line_slope','day_thick_line_green','day_hist_value','day_hist_green','week_thick_line_value','week_red_below_zero_turns_green']: df_indicators[col]=np.nan
    return df_indicators.dropna(subset=['ATR', 'ma200', 'rsi']) # Ensure key indicators are present

def detect_setup(df, idx=-1):
    # Sort DataFrame by index in descending order (newest first)
    df = df.sort_index(ascending=False)

    required_for_any_setup = ['rsi', 'close_vs_open', 'day_thick_line_green', 'ma5', 'ma20', 'ma50', 'ATR', 'Volume'] # Added 'Volume' just in case, can be removed if not used by specific setup logic directly
    
    missing_cols_for_setup = [col for col in required_for_any_setup if col not in df.columns]
    if missing_cols_for_setup:
        logger.debug(f"Cannot detect setup; DataFrame is missing required indicator columns: {missing_cols_for_setup}. This might be due to insufficient data for their calculation.")
        return False, None, None

    # If idx is -1 (default, meaning latest), current_candle_idx is 0 (first row after sort).
    # Otherwise, use the provided idx.
    current_candle_idx = 0 if idx == -1 else idx
    previous_candle_idx = current_candle_idx + 1

    # Boundary checks
    if not (0 <= current_candle_idx < len(df)):
        logger.debug(f"Cannot detect setup; current_candle_idx {current_candle_idx} is out of bounds for DataFrame of length {len(df)}.")
        return False, None, None

    if not (previous_candle_idx < len(df)):
        logger.debug(f"Cannot detect setup; previous_candle_idx {previous_candle_idx} is out of bounds for DataFrame of length {len(df)}. Not enough data for comparison.")
        return False, None, None

    # Ensure we have at least two rows of data for current and previous values after sorting
    if len(df) <= 1:
        logger.debug(f"Cannot detect setup; DataFrame has {len(df)} rows, need at least 2 for current and previous values.")
        return False, None, None

    current_values = {col: df[col].iloc[current_candle_idx] for col in required_for_any_setup}
    prev_values = {col: df[col].iloc[previous_candle_idx] for col in required_for_any_setup}
    
    nan_current = [k for k, v in current_values.items() if pd.isna(v)]
    nan_prev = [k for k, v in prev_values.items() if pd.isna(v)]

    if nan_current or nan_prev:
        logger.debug(f"Cannot detect setup; NaN values found in current ({nan_current}) or previous ({nan_prev}) indicator values after sorting.")
        return False, None, None
        
    setup_detected, setup_type, tier = False, None, None
    current_rsi, prev_rsi = current_values['rsi'], prev_values['rsi']
    current_close_vs_open, current_day_thick_line_green = current_values['close_vs_open'], current_values['day_thick_line_green']
    current_ma5, prev_ma5 = current_values['ma5'], prev_values['ma5']
    current_ma20, prev_ma20 = current_values['ma20'], prev_values['ma20']
    current_ma50, prev_ma50 = current_values['ma50'], prev_values['ma50']

    # Use current_candle_idx for current_volume_ratio
    current_volume_ratio = df['volume_ratio'].iloc[current_candle_idx] if 'volume_ratio' in df.columns and not pd.isna(df['volume_ratio'].iloc[current_candle_idx]) else np.nan

    if current_day_thick_line_green == 1 and current_rsi > 30 and prev_rsi <= 30:
        setup_detected, setup_type, tier = True, 'BOTTOM_TURN', 'high'
    elif (current_ma5 > current_ma20 and prev_ma5 <= prev_ma20) or \
         (current_ma5 > current_ma50 and prev_ma5 <= prev_ma50):
        setup_detected, setup_type, tier = True, 'MA_CROSS', 'medium'
    elif not pd.isna(current_volume_ratio) and not pd.isna(current_close_vs_open) and \
         ((current_volume_ratio > STRATEGY_PARAMS['volume_spike_threshold'] and current_close_vs_open > 0) or \
          (current_volume_ratio > 1.0 and current_close_vs_open > 0.01)): # Ensure positive price move for general volume spike
        setup_detected, setup_type, tier = True, 'VOLUME_SPIKE', 'low'
    # This last condition seems redundant or too similar to BOTTOM_TURN if current_close_vs_open > 0 implies a price increase from open.
    # And if it's about RSI turn, it's covered. If it's about general positive movement with RSI turn, it's also covered.
    # Let's assume the existing logic is preferred unless specified.
    # Original: elif not pd.isna(current_close_vs_open) and current_rsi > 30 and prev_rsi <= 30 and current_close_vs_open > 0:
    # This is essentially BOTTOM_TURN (rsi > 30 and prev_rsi <= 30) combined with a positive close_vs_open.
    # BOTTOM_TURN already captures the RSI condition. If close_vs_open is an additional filter, it should be part of BOTTOM_TURN or a new type.
    # For now, keeping it distinct as per original logic but noting potential overlap.
    elif not pd.isna(current_close_vs_open) and current_rsi > 30 and prev_rsi <= 30 and current_close_vs_open > 0.005: # Made threshold slightly more significant
        setup_detected, setup_type, tier = True, 'RSI_TURN_POSITIVE_CLOSE', 'low' # Renamed for clarity

    return setup_detected, setup_type, tier

def calculate_potential_trade_params(df, entry_idx=-1):
    # entry_idx here will correspond to current_candle_idx after sorting in detect_setup
    # So, if detect_setup passes idx=-1, it means current_candle_idx = 0 (latest)
    # This function will then also use df.iloc[0] if entry_idx is -1 (or 0)

    # If df is already sorted descending, and entry_idx is for the "current" candle (e.g. 0 for latest)
    effective_idx = 0 if entry_idx == -1 else entry_idx

    if not (0 <= effective_idx < len(df)): # Check bounds
        logger.warning(f"calculate_potential_trade_params: effective_idx {effective_idx} out of bounds for df of length {len(df)}")
        return None

    if not ('ATR' in df.columns and 'Close' in df.columns and 
            not pd.isna(df['ATR'].iloc[effective_idx]) and 
            not pd.isna(df['Close'].iloc[effective_idx])):
        logger.warning(f"calculate_potential_trade_params: Missing ATR/Close or NaN values at effective_idx {effective_idx}.")
        return None

    entry_price, atr_val = df['Close'].iloc[effective_idx], df['ATR'].iloc[effective_idx]

    if pd.isna(atr_val) or atr_val <= 0: # ATR must be positive
        logger.warning(f"calculate_potential_trade_params: Invalid ATR value ({atr_val}) at effective_idx {effective_idx}.")
        return None

    stop_loss_price = entry_price - (atr_val * STRATEGY_PARAMS['atr_multiplier'])
    target_price = entry_price + (atr_val * STRATEGY_PARAMS['atr_multiplier'] * STRATEGY_PARAMS['target_multiplier'])
    risk_reward_ratio = (target_price - entry_price) / (entry_price - stop_loss_price) if (entry_price - stop_loss_price) > 1e-9 else 0
    return {'entry_price': entry_price, 'stop_loss_price': stop_loss_price, 'target_price': target_price,
            'risk_reward_ratio': risk_reward_ratio, 'atr': atr_val}

def apply_high_probability_filter_live(potential_trade, setup_type, tier, params):
    # (Identical to previous live scanner but takes params)
    score = 0.0
    if potential_trade['risk_reward_ratio'] < params['risk_reward_ratio_threshold']: return False, 0.0
    score += min(potential_trade['risk_reward_ratio'] / 3.0, 1.0) * 40 
    if params['prioritize_volume_spike'] and setup_type == 'VOLUME_SPIKE': score += 35
    elif setup_type == 'BOTTOM_TURN': score += 25
    elif setup_type == 'MA_CROSS': score += 10
    if params['prioritize_low_tier'] and tier == 'low': score += 25
    elif tier == 'high': score += 15
    elif tier == 'medium': score += 10
    normalized_score = min(score / 100.0, 1.0)
    if normalized_score < params['win_rate_threshold']: # Use threshold from params
        if normalized_score > params['win_rate_threshold'] * 0.9 and np.random.random() < 0.05: return True, normalized_score
        return False, normalized_score
    return True, normalized_score

# --- Helper function for formatting signal output ---
def _format_signal_output(raw_signal_data: dict, market_segment_name: str, signal_found: bool = True) -> dict:
    """
    Formats a raw signal data dictionary into the standard output structure.
    Handles NaN/inf conversion, date formatting, key renaming, and adds market segment.
    """
    if not signal_found: # Handles "no signal found" cases for segments or overall
        current_date_str = datetime.now().strftime('%Y-%m-%d')
        message = raw_signal_data.get('message', f'No signal found today for {market_segment_name} market.')
        return {
            'signal_found': False,
            'date': current_date_str,
            'market_segment': market_segment_name,
            'message': message
        }

    # Handle data type conversions for historical performance
    hist_strength = raw_signal_data.get('hist_strength_score')
    if pd.isna(hist_strength) or hist_strength == -np.inf:
        hist_strength = None

    hist_win_rate = raw_signal_data.get('hist_win_rate')
    if pd.isna(hist_win_rate):
        hist_win_rate = None

    hist_total_trades = raw_signal_data.get('hist_total_trades')
    if pd.isna(hist_total_trades):
        hist_total_trades = None
    elif hist_total_trades is not None:
        try:
            hist_total_trades = int(hist_total_trades)
        except (ValueError, TypeError):
            logger.warning(f"Could not convert hist_total_trades '{hist_total_trades}' to int for symbol {raw_signal_data.get('symbol')}. Setting to None.")
            hist_total_trades = None
            
    # Format date: raw_signal_data['date'] is datetime.date object
    signal_date_str = raw_signal_data['date'].strftime('%Y-%m-%d') if isinstance(raw_signal_data.get('date'), datetime) or isinstance(raw_signal_data.get('date'), pd.Timestamp) or hasattr(raw_signal_data.get('date'), 'strftime') else str(raw_signal_data.get('date'))


    formatted_signal = {
        'signal_found': True,
        'date': signal_date_str,
        'market_segment': market_segment_name,
        'symbol': raw_signal_data['symbol'],
        'setup_type': raw_signal_data['setup_type'],
        'tier': raw_signal_data['tier'],
        'strategy_score': raw_signal_data['score'],  # Renamed from 'score'
        'historical_strength_score': hist_strength,
        'historical_win_rate': hist_win_rate,
        'historical_total_trades': hist_total_trades,
        'latest_close': raw_signal_data['latest_close'],
        'entry_price': raw_signal_data['entry_price'],
        'stop_loss_price': raw_signal_data['stop_loss_price'],
        'target_price': raw_signal_data['target_price'],
        'risk_reward_ratio': raw_signal_data['risk_reward_ratio'],
        'atr': raw_signal_data['atr']
    }
    return formatted_signal


def find_current_setups(params): # Pass current STRATEGY_PARAMS
    logger.info("Starting market scan for high probability setups...")
    symbols_by_market = load_symbols() # Returns a dictionary {'market': [symbols]}
    all_qualified_setups_with_market = [] # Store all setups found, including their market segment

    # Check if any symbols were loaded at all
    if not symbols_by_market or all(not symbols for symbols in symbols_by_market.values()):
        logger.error("No symbols loaded from any market to scan.")
        current_date_str = datetime.now().strftime('%Y-%m-%d')
        no_signal_overall = {
            'signal_found': False, 'date': current_date_str, 
            'message': 'No symbols loaded to scan.'
        }
        # For segmented_signals, create a "no symbols loaded" message for each expected market
        segmented_no_signals = {}
        for market_key in TICKER_FILES.keys(): # Use TICKER_FILES keys as the definitive list of expected markets
             segmented_no_signals[market_key] = {
                'signal_found': False, 'date': current_date_str, 
                'market_segment': market_key, 
                'message': f'No symbols loaded to scan for {market_key} market.'
            }
        return {'overall_top_signal': no_signal_overall, 'segmented_signals': segmented_no_signals}

    for market_segment, symbols_list in symbols_by_market.items():
        if not symbols_list:
            logger.info(f"No symbols to scan for {market_segment} market segment.")
            continue # Skip to the next market segment

        logger.info(f"Starting scan for {market_segment.upper()} with {len(symbols_list)} symbols.")
        for symbol in tqdm(symbols_list, desc=f"Scanning {market_segment.upper()}"):
            df_raw = fetch_stock_data_yf(symbol, period=DATA_FETCH_PERIOD)
            if df_raw is None:
                logger.debug(f"Symbol {symbol}: No data fetched.")
                continue
            
            df_indicators = calculate_indicators(df_raw)
            if df_indicators is None or df_indicators.empty or \
               len(df_indicators) < params['min_data_days'] or \
               pd.isna(df_indicators['Close'].iloc[-1]):
                logger.debug(f"Symbol {symbol}: Insufficient/invalid indicator data. Skipping.")
                continue
                
            setup_detected, setup_type, tier = detect_setup(df_indicators, idx=-1)
            if setup_detected:
                logger.debug(f"Symbol {symbol}: Setup DETECTED - Type: {setup_type}, Tier: {tier}")
                trade_params = calculate_potential_trade_params(df_indicators, entry_idx=-1)
                if trade_params is None: continue # Should also log this ideally, or ensure trade_params func logs
                
                passes_filter, score = apply_high_probability_filter_live(trade_params, setup_type, tier, params)
                if passes_filter:
                    logger.info(f"Symbol {symbol}: PASSED FILTER. Score: {score:.3f}, Setup: {setup_type}, Tier: {tier}")
                    setup_info = {
                        'symbol': symbol,
                        'date': df_indicators.index[-1].date(),
                        'setup_type': setup_type,
                        'tier': tier,
                        'score': score,
                        'market_segment': market_segment,
                        **trade_params,
                        'latest_close': df_indicators['Close'].iloc[-1],
                        'hist_strength_score': -np.inf,
                        'hist_win_rate': 0.0,
                        'hist_total_trades': 0
                    }
                    
                    if long_term_historical_perf_df is not None and not long_term_historical_perf_df.empty:
                        symbol_hist_data = long_term_historical_perf_df[long_term_historical_perf_df['symbol'] == symbol]
                        if not symbol_hist_data.empty:
                            setup_info['hist_strength_score'] = symbol_hist_data['hist_strength_score'].iloc[0]
                            setup_info['hist_win_rate'] = symbol_hist_data['hist_win_rate'].iloc[0]
                            setup_info['hist_total_trades'] = symbol_hist_data['hist_total_trades'].iloc[0]
                    
                    all_qualified_setups_with_market.append(setup_info)
                    # logger.debug(f"  Potentially QUALIFIED: {symbol} ({market_segment}) | Score: {score:.3f}") # Replaced by PASSED FILTER log
                else:
                    logger.debug(f"Symbol {symbol}: FAILED FILTER. Score: {score:.3f}, Setup: {setup_type}, Tier: {tier}")
            else:
                logger.debug(f"Symbol {symbol}: No setup detected.")

    # --- Signal Identification ---
    overall_top_signal_dict = {}
    segmented_signals_dict = {}
    
    logger.info(f"Total qualified setups found across all markets (before final selection): {len(all_qualified_setups_with_market)}")
    if all_qualified_setups_with_market:
        for s_debug in all_qualified_setups_with_market:
            logger.debug(f"  Qualified: {s_debug['symbol']}, Score: {s_debug['score']:.3f}, HistStrength: {s_debug.get('hist_strength_score', -np.inf):.2f}, Market: {s_debug['market_segment']}")

    # Determine if historical performance data is available for tie-breaking
    hist_data_available = (
        long_term_historical_perf_df is not None
        and not long_term_historical_perf_df.empty
    )

    if hist_data_available:
        sort_key = lambda x: (x['score'], x.get('hist_strength_score', -np.inf))
    else:
        logger.info(
            "No long-term historical stats available; using latest close as a tie-breaker."
        )
        sort_key = lambda x: (x['score'], x.get('latest_close', 0.0), x['symbol'])

    # Overall Top Signal
    if all_qualified_setups_with_market:
        all_qualified_setups_with_market.sort(key=sort_key, reverse=True)
        overall_top_signal_raw = all_qualified_setups_with_market[0]
        overall_top_signal_dict = _format_signal_output(overall_top_signal_raw, overall_top_signal_raw['market_segment'])
        logger.info(f"Overall top signal selected: {overall_top_signal_dict.get('symbol')} with score {overall_top_signal_dict.get('strategy_score')}. Full details: {overall_top_signal_dict}")

    else:
        logger.info("No overall top signal selected after filtering and sorting.") # Changed from "No signals found today..."
        overall_top_signal_dict = _format_signal_output(
            raw_signal_data={'message': 'No overall top signal selected after filtering and sorting.'}, # More specific message
            market_segment_name='overall',
            signal_found=False
        )
        overall_top_signal_dict['market_segment'] = 'N/A'

    # Per-Segment Top Signals
    for segment_key in TICKER_FILES.keys():
        segment_setups = [s for s in all_qualified_setups_with_market if s['market_segment'] == segment_key]
        if segment_setups:
            segment_setups.sort(key=sort_key, reverse=True)
            top_signal_for_segment_raw = segment_setups[0]
            formatted_segment_signal = _format_signal_output(top_signal_for_segment_raw, segment_key)
            segmented_signals_dict[segment_key] = formatted_segment_signal
            logger.info(f"Top signal for {segment_key.upper()} market selected: {formatted_segment_signal.get('symbol')} with score {formatted_segment_signal.get('strategy_score')}. Details: {formatted_segment_signal}")
        else:
            logger.info(f"No top signal selected for {segment_key.upper()} market after filtering and sorting.")
            segmented_signals_dict[segment_key] = _format_signal_output(
                raw_signal_data={'message': f'No top signal selected for {segment_key} market after filtering and sorting.'}, # More specific
                market_segment_name=segment_key,
                signal_found=False
            )
            
    return {'overall_top_signal': overall_top_signal_dict, 'segmented_signals': segmented_signals_dict}

def print_trade_summary_and_distribution(all_qualified_setups, score_bins):
    # (Identical to previous live scanner)
    total_setups = len(all_qualified_setups)
    logger.info(f"\\n--- Trade Candidate Summary (Today) ---")
    logger.info(f"Total qualified setups found today (before daily cap): {total_setups}")
    if total_setups == 0: logger.info("No setups met the criteria today."); logger.info("--- End of Summary ---"); return
    # Explicitly type bin_counts to satisfy the type checker
    bin_counts: OrderedDict[str, Any] = OrderedDict([(f"{l:.2f}-<{u:.2f}", 0) for l, u in score_bins])
    for setup_item in all_qualified_setups:
        score = setup_item['score']
        for lower, upper in score_bins:
            if lower <= score < upper: bin_counts[f"{lower:.2f}-<{upper:.2f}"] += 1; break
    logger.info("Distribution by Score Range (Today's Candidates):")
    has_trades_in_bins = False
    for bin_range, count in bin_counts.items():
        if count > 0: logger.info(f"  Score {bin_range}: {count} trade(s)"); has_trades_in_bins = True
    if not has_trades_in_bins: logger.info("  No trades fell into defined score bins.")
    logger.info("--- End of Summary ---")

def generate_json_output(signal_data_complex: dict):
    """
    Generates and outputs/saves a JSON representation of the complex signal data
    (overall and segmented signals).

    If running in GitHub Actions (GITHUB_ACTIONS env var is 'true'),
    prints JSON to stdout. Otherwise, saves to a file in
    PROJECT_BASE_DIR/results/live_signals/ with filename daily_signal_YYYY-MM-DD.json.
    """
    try:
        # Serialize the entire complex dictionary
        json_output = json.dumps(signal_data_complex, indent=4)
    except TypeError as e:
        logger.error(f"Error serializing complex signal data to JSON: {e}")
        return

    if os.environ.get('GITHUB_ACTIONS') == 'true':
        logger.info("Printing complex signal data (overall and segmented) to stdout for GitHub Action...")
        print(json_output)
    else:
        # Local execution: save to file
        output_dir_name = "live_signals"
        output_dir = Path(PROJECT_BASE_DIR) / "results" / output_dir_name
        
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Determine date for filename from the complex data structure
            signal_date_str = None
            overall_signal = signal_data_complex.get('overall_top_signal', {})
            
            if overall_signal.get('signal_found') and isinstance(overall_signal.get('date'), str):
                signal_date_str = overall_signal['date']
            else: # Try segmented signals if overall doesn't have a valid date
                segmented_signals = signal_data_complex.get('segmented_signals', {})
                for market_data in segmented_signals.values():
                    if market_data.get('signal_found') and isinstance(market_data.get('date'), str):
                        signal_date_str = market_data['date']
                        break # Found a date from a segmented signal
            
            if not signal_date_str: # Fallback to current date
                logger.warning("Could not determine signal date from overall or segmented signals; using current date for filename.")
                signal_date_str = datetime.now().strftime('%Y-%m-%d')

            # Validate the determined date format
            try:
                datetime.strptime(signal_date_str, '%Y-%m-%d') # Validates format
            except ValueError:
                logger.error(f"Invalid date format '{signal_date_str}' determined for filename. Using current date as fallback.")
                signal_date_str = datetime.now().strftime('%Y-%m-%d')

            filename = f"daily_signal_{signal_date_str}.json"
            filepath = output_dir / filename
            
            with open(filepath, 'w') as f:
                f.write(json_output)
            logger.info(f"Saving complex signal data (overall and segmented) to file: {filepath}")
            
        except IOError as e:
            logger.error(f"IOError saving JSON to file {filepath}: {e}")
        except Exception as e: 
            logger.error(f"Unexpected error saving JSON to file {filepath if 'filepath' in locals() else 'unknown'}: {e}")

def main():
    logger.info("Starting High Probability Live Signal Generator (Refined)...")
    
    load_long_term_historical_stats() # Load historical performance for tie-breaking
    
    # `create_dummy_ticker_files()` is not needed if ticker files are managed externally
    # If you want it for first-time setup, ensure paths in it are also correct.
    
    # Step 1: Find Signal
    signal_data = find_current_setups(STRATEGY_PARAMS) # This returns the dictionary
    
    # Step 2: Generate JSON Output
    generate_json_output(signal_data) # signal_data here is the complex_signal_data

    # Step 3: Produce Human-Readable Console Output
    logger.info("Generating human-readable console output...")
    _print_signal_details_to_console(
        signal_data.get('overall_top_signal', {}),
        "--- Overall Top Selected Trade Candidate for Today ---"
    )

    print("\n\n--- Per-Segment Top Signal Summary ---") # Use print for console separation
    logger.info("Generating per-segment console output...")
    segmented_signals = signal_data.get('segmented_signals', {})
    if segmented_signals:
        for market_name, segment_signal_data in segmented_signals.items():
            _print_signal_details_to_console(
                segment_signal_data,
                f"--- Top Signal for {market_name.upper()} Market ---"
            )
    else:
        print("No per-segment signal data available.")
        logger.info("No per-segment signal data was available to print.")

    # Note for user, if an overall signal was found
    overall_signal = signal_data.get('overall_top_signal', {})
    if overall_signal.get('signal_found'):
         logger.info("\nNote: Weekly trade limit (e.g., max 3) should be managed by the user based on daily signals.")
            
    logger.info("\nLive Signal Generator finished.")
    logger.info("Disclaimer: This is NOT financial advice. Data from yfinance can have delays. Always do your own research.")

# --- Helper for console printing ---
def _print_signal_details_to_console(signal_dict: dict, signal_title: str):
    """
    Prints the details of a single signal dictionary (overall or segmented) to the console.
    """
    logger.debug(f"Printing details to console: '{signal_title}'")
    print(f"\n{signal_title}") # Use print for console output

    if not signal_dict: # Handle cases where the signal_dict itself might be empty
        print("  No signal data provided for this section.")
        return

    if signal_dict.get('signal_found'):
        # Market segment for overall signal might be 'N/A' if no signals found at all,
        # or the actual market if a signal was found.
        market_segment_info = ""
        if 'market_segment' in signal_dict and signal_dict['market_segment'] != 'N/A' and "Overall" in signal_title:
             market_segment_info = f" (Market: {signal_dict['market_segment']})"
        
        print(f"  Symbol: {signal_dict.get('symbol', 'N/A')}{market_segment_info}")
        print(f"  Date: {signal_dict.get('date', 'N/A')}")
        print(f"  Setup Type: {signal_dict.get('setup_type', 'N/A')} (Tier: {signal_dict.get('tier', 'N/A')})")
        
        strategy_score = signal_dict.get('strategy_score')
        if strategy_score is not None:
            print(f"  Strategy Score: {strategy_score:.3f}")
        else:
            print(f"  Strategy Score: N/A")

        hist_strength = signal_dict.get('historical_strength_score')
        hist_win_rate = signal_dict.get('historical_win_rate')
        hist_trades = signal_dict.get('historical_total_trades')

        if hist_strength is not None and hist_win_rate is not None and hist_trades is not None:
            print(f"  Historical Strength: {hist_strength:.2f} (Win Rate: {hist_win_rate:.2%}, Trades: {hist_trades})")
        else:
            na_details = []
            if hist_strength is None: na_details.append("Strength=N/A")
            if hist_win_rate is None: na_details.append("WinRate=N/A")
            if hist_trades is None: na_details.append("Trades=N/A")
            print(f"  Historical Strength: N/A ({', '.join(na_details)})")
            
        latest_close = signal_dict.get('latest_close')
        print(f"  Current Close: {latest_close:.2f}" if latest_close is not None else "  Current Close: N/A")
        
        entry_price = signal_dict.get('entry_price')
        print(f"  Potential Entry: ~{entry_price:.2f}" if entry_price is not None else "  Potential Entry: N/A")

        sl_price = signal_dict.get('stop_loss_price')
        print(f"  Potential Stop-Loss: {sl_price:.2f}" if sl_price is not None else "  Potential Stop-Loss: N/A")
        
        tp_price = signal_dict.get('target_price')
        print(f"  Potential Target: {tp_price:.2f}" if tp_price is not None else "  Potential Target: N/A")

        rr_ratio = signal_dict.get('risk_reward_ratio')
        print(f"  Potential R:R: {rr_ratio:.2f}" if rr_ratio is not None else "  Potential R:R: N/A")
        
        atr_val = signal_dict.get('atr')
        print(f"  ATR: {atr_val:.3f}" if atr_val is not None else "  ATR: N/A")
    else:
        # Print the message for "no signal found" cases
        # The market_segment is already part of the message from _format_signal_output
        print(f"  {signal_dict.get('message', 'No signal information available for this section.')}")


if __name__ == "__main__":
    # The example call to find_current_setups and json.dumps is removed from the active main execution path.
    # It can be kept commented out for debugging purposes if desired.
    # # Example: To see the output of find_current_setups directly for testing
    # # result = find_current_setups(STRATEGY_PARAMS)
    # # import json
    # # print(json.dumps(result, indent=4, default=str)) # Print the dict
    main()
